# 第 9 节 Python 标准库（中）

作者：Vamei 出处：http://www.cnblogs.com/vamei 欢迎转载，也请保留这段声明。谢谢！

## 一、子进程 (subprocess 包)

这里的内容以 Linux 进程基础和 Linux 文本流为基础。subprocess 包主要功能是**执行外部的命令和程序**。比如说，我需要使用 wget 下载文件。我在 Python 中调用 wget 程序。从这个意义上来说，subprocess 的功能与 shell 类似。

### 1、subprocess 以及常用的封装函数

当我们运行 python 的时候，我们都是在创建并运行一个进程。一个进程可以 fork 一个子进程，并让这个子进程 exec 另外一个程序。在 Python 中，我们通过标准库中的 subprocess 包来 fork 一个子进程，并运行一个外部的程序。

subprocess 包中定义有数个创建子进程的函数，这些函数分别以不同的方式创建子进程，所以我们可以根据需要来从中选取一个使用。另外 subprocess 还提供了一些管理标准流(standard stream)和管道(pipe)的工具，从而在进程间使用**文本通信**。

使用 subprocess 包中的函数创建子进程的时候，要注意:

*   1 在创建子进程之后，父进程是否暂停，并等待子进程运行。

*   2 函数返回什么

*   3 当 returncode 不为 0 时，父进程如何处理。

(1)、subprocess.call()

父进程等待子进程完成

返回**退出信息**(returncode，相当于 exit code)

(2)、subprocess.check_call()

父进程等待子进程完成

返回 0

检查退出信息，如果 returncode 不为 0，则举出错误 subprocess.CalledProcessError，该对象包含有 returncode 属性，可用 try...except...来检查(见 Python 错误处理)。

(3)、subprocess.check_output()

父进程等待子进程完成

返回子进程向标准输出的**输出结果**

检查退出信息，如果 returncode 不为 0，则举出错误 subprocess.CalledProcessError，该对象包含有 returncode 属性和 output 属性，output 属性为标准输出的输出结果，可用 try...except...来检查。

这三个函数的使用方法相类似，我们以 subprocess.call()来说明:

```py
import subprocess
rc = subprocess.call(["ls","-l"]) 
```

我们将程序名(ls)和所带的参数(-l)一起放在一个表中传递给 subprocess.call()

可以通过一个 shell 来解释一整个字符串:

```py
import subprocess
out = subprocess.call("ls -l", shell=True)
out = subprocess.call("cd ..", shell=True) 
```

我们使用了**shell=True**这个参数。这个时候，我们使用**一整个字符串**，而**不是一个表**来运行子进程。Python 将**先运行一个 shell**，再用这个 shell 来解释这整个字符串。

shell 命令中有一些是 shell 的内建命令，这些命令必须通过 shell 运行，$cd。shell=True 允许我们运行这样一些命令。

### 2、Popen()

实际上，我们上面的三个函数都是基于**Popen()**的封装(wrapper)。这些封装的目的在于让我们容易使用子进程。当我们想要更个性化我们的需求的时候，就要转向 Popen 类，该类生成的对象用来代表子进程。

与上面的封装不同，Popen 对象创建后，主程序不会自动等待子进程完成。我们必须调用对象的**wait()**方法，父进程才会等待 (也就是阻塞 block)：

```py
import subprocess
child = subprocess.Popen(["ping","-c","5","www.google.com"])
print("parent process") 
```

从运行结果中看到，父进程在开启子进程之后并没有等待 child 的完成，而是直接运行 print。

对比等待的情况:

```py
import subprocess
child = subprocess.Popen(["ping","-c","5","www.google.com"])
child.wait()
print("parent process") 
```

此外，你还可以在父进程中对子进程进行其它操作，比如我们上面例子中的 child 对象:

*   child.poll() # 检查子进程状态

*   child.kill() # 终止子进程

*   child.send_signal() # 向子进程发送信号

*   child.terminate() # 终止子进程

子进程的 PID 存储在 child.pid。

### 3、子进程的文本流控制

(沿用 child 子进程) 子进程的标准输入，标准输出和标准错误也可以通过如下属性表示:

```py
child.stdin
child.stdout
child.stderr 
```

我们可以在 Popen()建立子进程的时候改变标准输入、标准输出和标准错误，并可以利用**subprocess.PIPE**将多个子进程的输入和输出连接在一起，构成**管道**(pipe):

```py
import subprocess
child1 = subprocess.Popen(["ls","-l"], stdout=subprocess.PIPE)
child2 = subprocess.Popen(["wc"], stdin=child1.stdout,stdout=subprocess.PIPE)
out = child2.communicate()
print(out) 
```

subprocess.PIPE 实际上为文本流提供一个缓存区。child1 的 stdout 将文本输出到缓存区，随后 child2 的 stdin 从该 PIPE 中将文本读取走。child2 的输出文本也被存放在 PIPE 中，直到 communicate()方法从 PIPE 中读取出 PIPE 中的文本。

要注意的是，**communicate()**是 Popen 对象的一个方法，该方法会阻塞父进程，直到子进程完成。

我们还可以利用 communicate()方法来使用 PIPE 给子进程输入:

```py
import subprocess
child = subprocess.Popen(["cat"], stdin=subprocess.PIPE)
child.communicate("vamei") 
```

我们启动子进程之后，cat 会等待输入，直到我们用 communicate()输入"vamei"。

**通过使用 subprocess 包，我们可以运行外部程序。这极大的拓展了 Python 的功能。如果你已经了解了操作系统的某些应用，你可以从 Python 中直接调用该应用(而不是完全依赖 Python)，并将应用的结果输出给 Python，并让 Python 继续处理。shell 的功能(比如利用文本流连接各个应用)，就可以在 Python 中实现。**

## 二、信号 (signal 包)

signal 包负责在 Python 程序内部处理信号，典型的操作包括预设信号处理函数，暂停并等待信号，以及定时发出 SIGALRM 等。要注意，signal 包主要是针对 UNIX 平台(比如 Linux, MAC OS)，而 Windows 内核中由于对信号机制的支持不充分，所以在 Windows 上的 Python 不能发挥信号系统的功能。

### 1、定义信号名

signal 包定义了各个信号名及其对应的整数，比如：

```py
import signal
print signal.SIGALRM
print signal.SIGCONT 
```

Python 所用的信号名和 Linux 一致。你可以通过以下命令查询：

```py
$man 7 signal 
```

### 2、预设信号处理函数

signal 包的核心是使用**signal.signal()**函数来**预设(register)信号处理函数**，如下所示：

```py
singnal.signal(signalnum, handler) 
```

signalnum 为某个信号，handler 为该信号的处理函数。我们在信号基础里提到，进程可以无视信号，可以采取默认操作，还可以自定义操作。当 handler 为**signal.SIG_IGN**时，信号被无视(ignore)。当 handler 为**singal.SIG_DFL**，进程采取默认操作(default)。当 handler 为一个函数名时，进程采取函数中定义的操作。

```py
import signal
# Define signal handler function
def myHandler(signum, frame):
    print('I received: ', signum)

# register signal.SIGTSTP's handler 
signal.signal(signal.SIGTSTP, myHandler)
signal.pause()
print('End of Signal Demo') 
```

在主程序中，我们首先使用**signal.signal()**函数来**预设**信号处理函数。然后我们执行**signal.pause()**来**让该进程暂停以等待信号**，以等待信号。当信号 SIGUSR1 被传递给该进程时，进程从暂停中恢复，并根据预设，执行 SIGTSTP 的信号处理函数 myHandler()。myHandler 的两个参数一个用来识别信号(signum)，另一个用来获得信号发生时，进程栈的状况(stack frame)。这两个参数都是由 signal.singnal()函数来传递的。

上面的程序可以保存在一个文件中(比如 test.py)。我们使用如下方法运行:

```py
$python test.py 
```

以便让进程运行。当程序运行到 signal.pause()的时候，进程暂停并等待信号。此时，通过按下 CTRL+Z 向该进程发送**SIGTSTP 信号**。我们可以看到，进程执行了 myHandle()函数, 随后返回主程序，继续执行。(当然，也可以用\$ps 查询 process ID, 再使用$kill 来发出信号。)

> (进程并不一定要使用 signal.pause()暂停以等待信号，它也可以在进行工作中接受信号，比如将上面的 signal.pause()改为一个需要长时间工作的循环。)

我们可以根据自己的需要更改 myHandler()中的操作，以针对不同的信号实现个性化的处理。

### 3、定时发出 SIGALRM 信号

一个有用的函数是**signal.alarm()**，它被用于在**一定时间之后**，向进程自身发送**SIGALRM**信号:

```py
import signal
# Define signal handler function
def myHandler(signum, frame):
    print("Now, it's the time")
    exit()

# register signal.SIGALRM's handler 
signal.signal(signal.SIGALRM, myHandler)
signal.alarm(5)
while True:
    print('not yet') 
```

我们这里用了一个无限循环以便让进程持续运行。在 signal.alarm()执行 5 秒之后，进程将向自己发出 SIGALRM 信号，随后，信号处理函数 myHandler 开始执行。

### 4、发送信号

signal 包的核心是设置信号处理函数。除了 signal.alarm()向自身发送信号之外，并没有其他发送信号的功能。但在 os 包中，有类似于 linux 的 kill 命令的函数，分别为：

```py
os.kill(pid, sid)

os.killpg(pgid, sid) 
```

分别向进程和进程组发送信号。sid 为信号所对应的整数或者 singal.SIG*。

实际上 signal, pause，kill 和 alarm 都是 Linux 应用编程中常见的 C 库函数，在这里，我们只不过是用 Python 语言来实现了一下。实际上，Python 的解释器是使用 C 语言来编写的，所以有此相似性也并不意外。此外，在 Python 3.4 中，signal 包被增强，信号阻塞等功能被加入到该包中。我们暂时不深入到该包中。

## 三、多线程与同步 (threading 包)

Python 主要通过标准库中的**threading**包来实现多线程。在当今**网络**时代，每个服务器都会接收到大量的请求。服务器可以利用多线程的方式来处理这些请求，以提高对网络端口的读写效率。Python 是一种网络服务器的后台工作语言，所以多线程也就很自然被 Python 语言支持。

> (关于多线程的原理和 C 实现方法，请参考[Linux 多线程与同步](http://www.cnblogs.com/vamei/archive/2012/10/09/2715393.html)，要了解 race condition, mutex 和 condition variable 的概念)

### 1、多线程售票以及同步

我们使用 Python 来实现 Linux 多线程与同步文中的售票程序。我们使用 mutex (也就是 Python 中的**Lock**类对象) 来实现线程的同步:

```py
# A program to simulate selling tickets in multi-thread way
# Written by Vamei

import threading
import time
import os

# This function could be any function to do other chores.
def doChore():
    time.sleep(0.5)

# Function for each thread
def booth(tid):
    global i
    global lock
    while True:
        lock.acquire()                # Lock; or wait if other thread is holding the lock
        if i != 0:
            i = i - 1                 # Sell tickets
            print(tid,':now left:',i) # Tickets left
            doChore()                 # Other critical operations
        else:
            print("Thread_id",tid," No more tickets")
            os._exit(0)              # Exit the whole process immediately
        lock.release()               # Unblock
        doChore()                    # Non-critical operations

# Start of the main function
i    = 100                           # Available ticket number 
lock = threading.Lock()              # Lock (i.e., mutex)

# Start 10 threads
for k in range(10):
    new_thread = threading.Thread(target=booth,args=(k,))   # Set up thread; target: the callable (function) to be run, args: the argument for the callable 
    new_thread.start()                                      # run the thread 
```

我们使用了两个全局变量，一个是 i，用以储存剩余票数；一个是 lock 对象，用于同步线程对 i 的修改。此外，在最后的 for 循环中，我们总共设置了 10 个线程。每个线程都执行 booth()函数。线程在调用 start()方法的时候正式启动 (实际上，计算机中最多会有 11 个线程，因为主程序本身也会占用一个线程)。Python 使用**threading.Thread**对象来代表线程，用**threading.Lock**对象来代表一个互斥锁 (mutex)。

有两点需要注意:

*   我们在函数中使用**global**来声明变量为全局变量，从而让多线程共享 i 和 lock (在 C 语言中，我们通过将变量放在所有函数外面来让它成为全局变量)。如果不这么声明，由于 i 和 lock 是**不可变数据对象**，它们将被当作一个局部变量。如果是**可变数据对象**的话，则不需要 global 声明。我们甚至可以将**可变数据对象**作为参数来传递给线程函数。这些线程将共享这些可变数据对象。

*   我们在 booth 中使用了两个**doChore()**函数。可以在未来改进程序，以便让线程除了进行 i=i-1 之外，做更多的操作，比如打印剩余票数，找钱，或者喝口水之类的。第一个 doChore()依然在 Lock 内部，所以可以安全地**使用共享资源** (critical operations, 比如打印剩余票数)。第二个 doChore()时，Lock 已经被释放，所以不能再去使用共享资源。这时候可以做一些**不使用共享资源**的操作 (non-critical operation, 比如找钱、喝水)。我故意让 doChore()等待了 0.5 秒，以代表这些额外的操作可能花费的时间。你可以定义的函数来代替 doChore()。

### 2、OOP 创建线程

上面的 Python 程序非常类似于一个面向过程的 C 程序。我们下面介绍如何通过**面向对象** (OOP， object-oriented programming) 的方法实现多线程，其核心是继承**threading.Thread**类。我们上面的 for 循环中已经利用了 threading.Thread()的方法来创建一个 Thread 对象，并将函数 booth()以及其参数传递给改对象，并调用 start()方法来运行线程。OOP 的话，通过修改 Thread 类的**run()**方法来定义线程所要执行的命令。

```py
# A program to simulate selling tickets in multi-thread way
# Written by Vamei

import threading
import time
import os

# This function could be any function to do other chores.
def doChore():
    time.sleep(0.5)

# Function for each thread
class BoothThread(threading.Thread):
    def __init__(self, tid, monitor):
        self.tid          = tid
        self.monitor = monitor
        threading.Thread.__init__(self)
    def run(self):
        while True:
            monitor['lock'].acquire()                          # Lock; or wait if other thread is holding the lock
            if monitor['tick'] != 0:
                monitor['tick'] = monitor['tick'] - 1          # Sell tickets
                print(self.tid,':now left:',monitor['tick'])   # Tickets left
                doChore()                                      # Other critical operations
            else:
                print("Thread_id",self.tid," No more tickets")
                os._exit(0)                                    # Exit the whole process immediately
            monitor['lock'].release()                          # Unblock
            doChore()                                          # Non-critical operations

# Start of the main function
monitor = {'tick':100, 'lock':threading.Lock()}

# Start 10 threads
for k in range(10):
    new_thread = BoothThread(k, monitor)
    new_thread.start() 
```

我们自己定义了一个类**BoothThread**, 这个类**继承自 thread.Threading 类**。然后我们把上面的 booth()所进行的操作统统放入到 BoothThread 类的**run()**方法中。注意，我们没有使用全局变量声明 global，而是使用了一个**词典** monitor 存放全局变量，然后把词典作为参数传递给线程函数。由于词典是**可变数据对象**，所以当它被传递给函数的时候，函数所使用的依然是同一个对象，相当于被多个线程所共享。这也是多线程乃至于多进程编程的一个技巧 (应尽量避免上面的 global 声明的用法，因为它并不适用于 windows 平台)。

上面 OOP 编程方法与面向过程的编程方法相比，并没有带来太大实质性的差别。

### 3、其他

**threading.Thread**对象： 我们已经介绍了该对象的 start()和 run(), 此外：

*   **join()**方法，调用该方法的线程将等待直到改 Thread 对象完成，再恢复运行。这与进程间调用 wait()函数相类似。

下面的对象用于处理**多线程同步**。对象一旦被建立，可以被多个线程共享，并根据情况阻塞某些进程。

**threading.Lock**对象: mutex, 有 acquire()和 release()方法。

**threading.Condition**对象: condition variable，建立该对象时，会包含一个 Lock 对象 (因为 condition variable 总是和 mutex 一起使用)。可以对 Condition 对象调用 acquire()和 release()方法，以控制潜在的 Lock 对象。此外:

*   **wait()**方法，相当于 cond_wait()

*   **notify*all()***，相当与 condbroadcast()

*   **nofify()**，与 notify_all()功能类似，但只唤醒一个等待的线程，而不是全部

**threading.Semaphore**对象: semaphore，也就是计数锁(semaphore 传统意义上是一种进程间同步工具)。创建对象的时候，可以传递一个整数作为**计数上限** (sema = threading.Semaphore(5))。它与 Lock 类似，也有 Lock 的两个方法。

**threading.Event**对象: 与 threading.Condition 相类似，相当于没有潜在的 Lock 保护的 condition variable。对象有 True 和 False 两个状态。可以多个线程使用 wait()等待，直到某个线程调用该对象的**set()**方法，将对象设置为 True。线程可以调用对象的**clear()**方法来重置对象为 False 状态。

## 四、进程信息 (部分 os 包)

Python 的 os 包中有查询和修改**进程信息**的函数。学习 Python 的这些工具也有助于理解 Linux 体系。

### 1、进程信息

os 包中相关函数如下：

(1)、uname() 返回操作系统相关信息。类似于 Linux 上的 uname 命令。

(2)、umask() 设置该进程创建文件时的权限 mask。类似于 Linux 上的 umask 命令

(3)、get*() 查询 (*由以下代替)

```py
uid, euid, resuid, gid, egid, resgid ：权限相关，其中 resuid 主要用来返回 saved UID

pid, pgid, ppid, sid                 ：进程相关 
```

(4)、put*() 设置 (*由以下代替)

```py
euid, egid： 用于更改 euid，egid。

uid, gid  ： 改变进程的 uid, gid。只有 super user 才有权改变进程 uid 和 gid (意味着要以$sudo python 的方式运行 Python)。

pgid, sid ： 改变进程所在的进程组(process group)和会话(session)。 
```

(5)、getenviron()：获得进程的环境变量

(6)、setenviron()：更改进程的环境变量

例 1，进程的 real UID 和 real GID：

```py
import os
print(os.getuid())
print(os.getgid()) 
```

将上面的程序保存为 py_id.py 文件，分别用\$python py_id.py 和\$sudo python py_id.py 看一下运行结果。

### 2、saved UID 和 saved GID

我们希望 saved UID 和 saved GID 如我们在[Linux 用户与“最小权限”原则](http://www.cnblogs.com/vamei/archive/2012/10/07/2713593.html)中描述的那样工作，但这很难。原因在于，当我们写一个 Python 脚本后，我们**实际运行的是 python 这个解释器**，而不是 Python 脚本文件。对比 C，C 语言直接运行由 C 语言编译成的执行文件。我们必须更改 python 解释器本身的权限来运用 saved UID 机制，然而这么做又是**异常危险**的。

比如说，我们的 python 执行文件为/usr/bin/python (你可以通过$which python 获知)

我们先看一下：

```py
$ls -l /usr/bin/python 
```

的结果：

```py
-rwxr-xr-x root root 
```

我们修改权限以设置 set UID 和 set GID 位 (参考[Linux 用户与“最小权限”原则](http://www.cnblogs.com/vamei/archive/2012/10/07/2713593.html))：

```py
$sudo chmod 6755 /usr/bin/python

/usr/bin/python 的权限成为:

-rwsr-sr-x root root 
```

随后，我们运行文件下面 test.py 文件，这个文件可以是由普通用户所有:

```py
import os
print(os.getresuid()) 
```

我们得到结果：(1000, 0, 0)

上面分别是 UID，EUID，saved UID。我们只用执行一个由普通用户拥有的 python 脚本，就可以得到 super user 的权限！所以，这样做是极度危险的，我们相当于交出了系统的保护系统。想像一下 Python 强大的功能，别人现在可以用这些强大的功能作为攻击你的武器了！使用下面命令来恢复到从前:

```py
$sudo chmod 0755 /usr/bin/python 
```

关于脚本文件的 saved UID/GID，更加详细的讨论见：http://www.faqs.org/faqs/unix-faq/faq/part4/section-7.html

## 五、多进程初步 (multiprocessing 包)

我们已经见过了使用 subprocess 包来创建子进程，但这个包有两个很大的局限性：

(1) 我们总是让 subprocess 运行外部的程序，而不是运行一个 Python 脚本内部编写的函数。

(2) 进程间只通过管道进行文本交流。以上限制了我们将 subprocess 包应用到更广泛的多进程任务。(这样的比较实际是不公平的，因为 subprocessing 本身就是设计成为一个 shell，而不是一个多进程管理包)。

### 1、threading 和 multiprocessing

**multiprocessing**包是 Python 中的多进程管理包。与 threading.Thread 类似，它可以利用**multiprocessing.Process**对象来创建一个进程。该进程可以运行在 Python 程序内部编写的函数。该 Process 对象与 Thread 对象的用法相同，也有 start(), run(), join()的方法。此外 multiprocessing 包中也有**Lock/Event/Semaphore/Condition**类 (这些对象可以像多线程那样，通过参数传递给各个进程)，用以**同步**进程，其用法与 threading 包中的同名类一致。所以，multiprocessing 的很大一部份与 threading 使用同一套 API，只不过换到了多进程的情境。

但在使用这些共享 API 的时候，我们要注意以下几点:

*   在 UNIX 平台上，当某个进程终结之后，该进程需要被其父进程调用 wait，否则进程成为僵尸进程(Zombie)。所以，有必要对每个 Process 对象调用**join()**方法 (实际上等同于 wait)。对于多线程来说，由于只有一个进程，所以不存在此必要性。

*   multiprocessing 提供了 threading 包中没有的 IPC(比如 Pipe 和 Queue)，效率上更高。应优先考虑 Pipe 和 Queue，**避免使用** Lock/Event/Semaphore/Condition 等同步方式 (因为它们占据的不是用户进程的资源)。

*   多进程应该**避免共享资源**。在多线程中，我们可以比较容易地共享资源，比如使用全局变量或者传递参数。在多进程情况下，由于每个进程有自己独立的内存空间，以上方法并不合适。此时我们可以通过共享内存和**Manager**的方法来共享资源。但这样做提高了程序的复杂度，并因为同步的需要而降低了程序的效率。

Process.PID 中保存有 PID，如果进程还没有 start()，则 PID 为 None。

我们可以从下面的程序中看到 Thread 对象和 Process 对象在使用上的相似性与结果上的不同。各个线程和进程都做一件事：打印 PID。但问题是，所有的任务在打印的时候都会向同一个标准输出(stdout)输出。这样输出的字符会混合在一起，无法阅读。使用 Lock 同步，在一个任务输出完成之后，再允许另一个任务输出，可以避免多个任务同时向终端输出。

```py
# Similarity and difference of multi thread vs. multi process
# Written by Vamei

import os
import threading
import multiprocessing

# worker function
def worker(sign, lock):
    lock.acquire()
    print(sign, os.getpid())
    lock.release()

# Main
print('Main:',os.getpid())

# Multi-thread
record = []
lock  = threading.Lock()
for i in range(5):
    thread = threading.Thread(target=worker,args=('thread',lock))
    thread.start()
    record.append(thread)

for thread in record:
    thread.join()

# Multi-process
record = []
lock = multiprocessing.Lock()
for i in range(5):
    process = multiprocessing.Process(target=worker,args=('process',lock))
    process.start()
    record.append(process)

for process in record:
    process.join() 
```

所有 Thread 的 PID 都与主程序相同，而每个 Process 都有一个不同的 PID。

### 2、Pipe 和 Queue

管道 PIPE 和消息队列 message queue，multiprocessing 包中有 Pipe 类和 Queue 类来分别支持这两种 IPC 机制。**Pipe**和**Queue**可以用来传送常见的对象。

#### (1)、 Pipe 可以是**单向**(half-duplex)，也可以是**双向**(duplex)。我们通过**mutiprocessing.Pipe(duplex=False)**创建单向管道 (默认为双向)。一个进程从 PIPE 一端输入对象，然后被 PIPE 另一端的进程接收，单向管道只允许管道一端的进程输入，而双向管道则允许从两端输入。

下面的程序展示了 Pipe 的使用:

```py
# Multiprocessing with Pipe
# Written by Vamei

import multiprocessing as mul

def proc1(pipe):
    pipe.send('hello')
    print('proc1 rec:',pipe.recv())

def proc2(pipe):
    print('proc2 rec:',pipe.recv())
    pipe.send('hello, too')

# Build a pipe
pipe = mul.Pipe()

# Pass an end of the pipe to process 1
p1   = mul.Process(target=proc1, args=(pipe[0],))
# Pass the other end of the pipe to process 2
p2   = mul.Process(target=proc2, args=(pipe[1],))
p1.start()
p2.start()
p1.join()
p2.join() 
```

这里的 Pipe 是双向的。

Pipe 对象建立的时候，返回一个含有两个元素的表，每个元素代表 Pipe 的一端(Connection 对象)。我们对 Pipe 的某一端调用**send()**方法来传送对象，在另一端使用**recv()**来接收。

#### (2)、 Queue 与 Pipe 相类似，都是先进先出的结构。但 Queue 允许多个进程放入，多个进程从队列取出对象。Queue 使用**mutiprocessing.Queue(maxsize)**创建，maxsize 表示队列中可以存放对象的最大数量。

下面的程序展示了 Queue 的使用:

```py
# Written by Vamei
import os
import multiprocessing
import time
#==================
# input worker
def inputQ(queue):
    info = str(os.getpid()) + '(put):' + str(time.time())
    queue.put(info)

# output worker
def outputQ(queue,lock):
    info = queue.get()
    lock.acquire()
    print (str(os.getpid()) + '(get):' + info)
    lock.release()
#===================
# Main
record1 = []   # store input processes
record2 = []   # store output processes
lock  = multiprocessing.Lock()    # To prevent messy print
queue = multiprocessing.Queue(3)

# input processes
for i in range(10):
    process = multiprocessing.Process(target=inputQ,args=(queue,))
    process.start()
    record1.append(process)

# output processes
for i in range(10):
    process = multiprocessing.Process(target=outputQ,args=(queue,lock))
    process.start()
    record2.append(process)

for p in record1:
    p.join()

queue.close()  # No more object will come, close the queue

for p in record2:
    p.join() 
```

一些进程使用**put()**在 Queue 中放入字符串，这个字符串中包含 PID 和时间。另一些进程从 Queue 中取出，并打印自己的 PID 以及**get()**的字符串。

## 作业

#### 1、参照[Linux 多线程与同步](http://www.cnblogs.com/vamei/archive/2012/10/09/2715393.html)中的 condition variable 的例子，使用 Python 实现。同时考虑使用面向过程和面向对象的编程方法。

#### 2、使用 mutiprocessing 包将本节内容中“多线程与同步 (threading 包)”的多线程程序更改为多进程程序。